# 插件

## ES 插件

[官方文档](https://www.elastic.co/guide/en/elasticsearch/plugins/7.6/_other_command_line_parameters.html)

### ES 插件分类

ES 提供插件机制对系统进行扩展

*   DIscovery Plugin
*   Analysis Plugin
*   Security Plugin
*   Management Plugin
*   Ingest Plugin
*   Mapper Plugin
*   Backup Plugin

### 查看已安装的插件

```bash
bin/elasticsearch-plugin list
```

也可以使用 ES 提供的 HTTP RESTful 接口查看

```
http://localhost:9200/_cat/plugins
```

显示如下：

```
2762956de850 analysis-icu 7.6.2
```

第一部分代表计算机，这里使用 Docker 安装的，所以显示的是容器 ID



### 安装插件

```bash
bin/elasticsearch-plugin install analysis-icu
```

若遇到速度太慢，可以使用代理。Mac 下 SSR 开始全局就行了。

```bash
ES_JAVA_OPTS="-Dhttp.proxyHost=127.0.0.1 -Dhttp.proxyPort=1086 -Dhttps.proxyHost=127.0.0.1 -Dhttps.proxyPort=1086" bin/elasticsearch-plugin install analysis-icu
```

若是在容器内，则需要修改 host 为宿主机的 host！

Windows 代理这样设置

```bash
set ES_JAVA_OPTS="-Dhttp.proxyHost=127.0.0.1 -Dhttp.proxyPort=1080 -Dhttps.proxyHost=127.0.0.1 -Dhttps.proxyPort=1080"
bin\elasticsearch-plugin install analysis-icu
```

::: tip 注意

安装完插件需要重启该节点

:::



### 卸载插件

```bash
bin/elasticsearch-plugin remove analysis-icu
```







## Kibana 插件

### 查看已安装的插件

```bash
bin/kibana-plugin list
```



### 安装插件

```bash
bin/kibana-plugin install plugin_location
```



### 卸载插件

```bash
bin/kibana-plugin remove plugin_location
```







## ES—IK 分词器

**添加文档时会进行分词**，**索引中存放的是一个个的词（term）**，当你去搜索时就是拿关键字去匹配词，最终找到词关联的文档。

测试当前索引库使用的分词器：

```bash
curl -XPOST "http://localhost:9200/_analyze" -H 'Content-Type: application/json' -d'
{  
"text": "中华人民共和国人民大会堂"
}
'
```

会发现当前索引库使用的分词器对中文就是**单字分词**



### 安装IK分词器

使用 IK 分词器可以实现对中文分词的效果。IK分词器 [Github 仓库](https://github.com/medcl/elasticsearch-analysis-ik)

可以下载压缩包；也可以使用 ES plugin 安装。注意与 Elasticsearch 版本统一！！！

```bash
./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.6.2/elasticsearch-analysis-ik-7.6.2.zip
```

重启 ES 后测试：

`post localhost:9200/_analyze`

```json
{"text":"中华人民共和国人民大会堂","analyzer":"ik_max_word"}
```



### 两种分词模式

IK分词器有两种分词模式：`ik_max_word`和`ik_smart`模式

* `ik_max_word`会将文本做**最细粒度**的拆分

    比如会将“**中华人民共和国人民大会堂**”拆分为“中华人民共和国、中华人民、中华、华人、人民共和国、人民、共和国、大会堂、大会、会堂等词语。

* `ik_smart`会做**最粗粒度**的拆分

    比如会将“中华人民共和国人民大会堂”拆分为中华人民共和国、人民大会堂



### 自定义词库

如果要让分词器支持一些专有词语，可以自定义如：**扩展词字典**、**停止词字典**（敏感词汇）

可以在 {conf}/analysis-ik/config/IKAnalyzer.cfg.xml 或 {plugins}/elasticsearch-analysis-ik-*/config/IKAnalyzer.cfg.xml 找到配置文件。该目录中有如下文件：

```
-rw-rw----  1 conanan  staff   625B  4 20 23:56 IKAnalyzer.cfg.xml
-rw-rw----  1 conanan  staff   5.0M  4 20 23:56 extra_main.dic
-rw-rw----  1 conanan  staff    62K  4 20 23:56 extra_single_word.dic
-rw-rw----  1 conanan  staff    62K  4 20 23:56 extra_single_word_full.dic
-rw-rw----  1 conanan  staff    11K  4 20 23:56 extra_single_word_low_freq.dic
-rw-rw----  1 conanan  staff   156B  4 20 23:56 extra_stopword.dic
-rw-rw----  1 conanan  staff   2.9M  4 20 23:56 main.dic
-rw-rw----  1 conanan  staff   123B  4 20 23:56 preposition.dic
-rw-rw----  1 conanan  staff   1.8K  4 20 23:56 quantifier.dic
-rw-rw----  1 conanan  staff   164B  4 20 23:56 stopword.dic
-rw-rw----  1 conanan  staff   192B  4 20 23:56 suffix.dic
-rw-rw----  1 conanan  staff   752B  4 20 23:56 surname.dic
```

修改该配置：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
	<comment>IK Analyzer 扩展配置</comment>
	<!--用户可以在这里配置自己的扩展字典 -->
	<entry key="ext_dict">mydict/ext_dict.dic</entry>
	 <!--用户可以在这里配置自己的扩展停止词字典-->
	<entry key="ext_stopwords">mydict/ext_stopwords.dic</entry>
	<!--用户可以在这里配置远程扩展字典 -->
	<!-- <entry key="remote_ext_dict">words_location</entry> -->
	<!--用户可以在这里配置远程扩展停止词字典-->
	<!-- <entry key="remote_ext_stopwords">words_location</entry> -->
</properties>
```

创建 mydict/ext_dict.dic 和 mydict/ext_stopwords.dic 文件。格式为 UTF-8（不要选择 UTF-8 BOM）

```
mydict/ext_dict.dic
风男
亚索
```

```
mydict/ext_stopwords.dic
傻
逼
傻逼
```

然后重启Elasticsearch，测试上述分词，即可看到效果：风男、亚索也被认为是一个完整的词。在停止词词典中的词不会被分词！

```bash
curl -XPOST "http://localhost:9200/_analyze" -H 'Content-Type: application/json' -d'
{  
	"text": "快乐风男！亚索傻逼！",  "analyzer": "ik_max_word"
}
'
```



